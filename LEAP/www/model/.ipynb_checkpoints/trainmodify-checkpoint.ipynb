{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3cd2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from www.utils import format_time\n",
    "import numpy as np\n",
    "from transformers import RobertaForMultipleChoice\n",
    "import progressbar\n",
    "from www.model.eval import evaluate_tiered\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "# Train a PyTorch model for one epoch\n",
    "def ComputeLoss(out1,out2):\n",
    "    loss_weights=[0.0, 0.4, 0.4, 0.2, 0.0]\n",
    "    total_loss = 0.0\n",
    "    total_loss += loss_weights[1] * out1['loss_preconditions'] / 20\n",
    "    total_loss += loss_weights[2] * out1['loss_effects']  / 20\n",
    "    total_loss += loss_weights[3] * out2['loss_conflicts']\n",
    "    total_loss += loss_weights[4] * out2['loss_stories']\n",
    "\n",
    "    return total_loss\n",
    "def train_epoch(model,\n",
    "                optimizer,\n",
    "                train_dataloader,\n",
    "                device,\n",
    "                list_output=False,\n",
    "                num_outputs=1,\n",
    "                span_mode=False,\n",
    "                seg_mode=False,\n",
    "                classifier=None,\n",
    "                multitask_idx=None):\n",
    "    t0 = time.time()\n",
    "\n",
    "    if not list_output:\n",
    "        total_loss = 0\n",
    "    else:\n",
    "        total_loss = [0 for _ in range(num_outputs)]\n",
    "\n",
    "    # Training mode\n",
    "    model.train()\n",
    "\n",
    "    if len(train_dataloader) * train_dataloader.batch_size >= 2500:\n",
    "        progress_update = True\n",
    "    else:\n",
    "        progress_update = False\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update\n",
    "        if progress_update and step % 50 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('\\t(%s) Starting batch %s of %s.' %\n",
    "                  (elapsed, str(step), str(len(train_dataloader))))\n",
    "\n",
    "        input_ids = batch[0].to(device)\n",
    "        input_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        # if input_ids.dim() > 2:\n",
    "        #   input_ids = input_ids.view(input_ids.shape[0], -1)\n",
    "        #   input_mask = input_mask.view(input_mask.shape[0], -1)\n",
    "\n",
    "        # In some cases, we also include a span for each training sequence which the model uses to classify only certain parts of the input\n",
    "        if span_mode:\n",
    "            spans = batch[3].to(device)\n",
    "        elif seg_mode:\n",
    "            segment_ids = batch[3].to(device)\n",
    "        else:\n",
    "            spans = None\n",
    "\n",
    "        # Forward pass\n",
    "        model.zero_grad()\n",
    "        if multitask_idx == None:\n",
    "            if span_mode:\n",
    "                out = model(input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=input_mask,\n",
    "                            labels=labels,\n",
    "                            spans=spans)\n",
    "            elif seg_mode:\n",
    "                out = model(input_ids,\n",
    "                            token_type_ids=segment_ids,\n",
    "                            attention_mask=input_mask,\n",
    "                            labels=labels)\n",
    "            else:\n",
    "                out = model(input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=input_mask,\n",
    "                            labels=labels)\n",
    "        else:\n",
    "            if span_mode:\n",
    "                out = model(input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=input_mask,\n",
    "                            labels=labels,\n",
    "                            spans=spans,\n",
    "                            task_idx=multitask_idx)\n",
    "            elif seg_mode:\n",
    "                out = model(input_ids,\n",
    "                            token_type_ids=segment_ids,\n",
    "                            attention_mask=input_mask,\n",
    "                            labels=labels,\n",
    "                            task_idx=multitask_idx)\n",
    "            else:\n",
    "                out = model(input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=input_mask,\n",
    "                            labels=labels,\n",
    "                            task_idx=multitask_idx)\n",
    "\n",
    "        if classifier != None:\n",
    "            sequence_output = out[0]\n",
    "            logits = classifier(out)\n",
    "\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                if self.num_labels == 1:\n",
    "                    #  We are doing regression\n",
    "                    loss_fct = MSELoss()\n",
    "                    loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "                elif self.num_labels == 2:\n",
    "                    loss_fct = CrossEntropyLoss()\n",
    "                    loss = loss_fct(logits.view(-1, self.num_labels),\n",
    "                                    labels.view(-1))\n",
    "\n",
    "        else:\n",
    "            loss = out[0]\n",
    "\n",
    "        # Backward pass\n",
    "        if not list_output:\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "        else:\n",
    "            for o in range(num_outputs):\n",
    "                total_loss[o] += loss[o].item()\n",
    "                loss[o].backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                       1.0)  # Gradient clipping\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    if list_output:\n",
    "        return list(np.array(total_loss) / len(train_dataloader)), model\n",
    "    else:\n",
    "        return total_loss / len(train_dataloader), model\n",
    "\n",
    "\n",
    "# Train a state classification pipeline for one epoch\n",
    "def train_epoch_tiered(model1,\n",
    "                       model2,\n",
    "                       optimizer,\n",
    "                       train_dataloader,\n",
    "                       device,\n",
    "                       seg_mode=False,\n",
    "                       return_losses=False,\n",
    "                       build_learning_curves=False,\n",
    "                       val_dataloader=None,\n",
    "                       train_lc_data=None,\n",
    "                       val_lc_data=None):\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    # Training mode\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    for layer in model1.precondition_classifiers:\n",
    "        layer.train()\n",
    "    for layer in model1.effect_classifiers:\n",
    "        layer.train()\n",
    "\n",
    "    # if len(train_dataloader) * train_dataloader.batch_size >= 2500:\n",
    "    #   progress_update = True\n",
    "    # else:\n",
    "    #   progress_update = False\n",
    "    progress_update = False\n",
    "\n",
    "    bar_size = len(train_dataloader)\n",
    "    bar = progressbar.ProgressBar(max_value=bar_size,\n",
    "                                  widgets=[\n",
    "                                      progressbar.Bar('#', '[', ']'), ' ',\n",
    "                                      progressbar.Percentage()\n",
    "                                  ])\n",
    "    bar_idx = 0\n",
    "    bar.start()\n",
    "\n",
    "    if train_lc_data is not None:\n",
    "        train_lc_data.append([])\n",
    "    if val_lc_data is not None:\n",
    "        val_lc_data.append([])\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update\n",
    "        if progress_update and step % 50 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('\\t(%s) Starting batch %s of %s.' %\n",
    "                  (elapsed, str(step), str(len(train_dataloader))))\n",
    "\n",
    "        input_ids = batch[0].long().to(device)\n",
    "        input_lengths = batch[1].to(device)  #.to(torch.int64).to('cpu')\n",
    "        input_entities = batch[2].to(device)\n",
    "        input_mask = batch[3].to(device)\n",
    "        attributes = batch[4].long().to(device)\n",
    "        preconditions = batch[5].long().to(device)\n",
    "        effects = batch[6].long().to(device)\n",
    "        conflicts = batch[7].long().to(device)\n",
    "        labels = batch[8].long().to(device)\n",
    "\n",
    "        if seg_mode:\n",
    "            segment_ids = batch[8].to(device)\n",
    "        else:\n",
    "            segment_ids = None\n",
    "\n",
    "        # Forward pass\n",
    "        model.zero_grad()\n",
    "        out_1 = classModel(input_ids, \n",
    "                    input_lengths,\n",
    "                    input_entities,\n",
    "                    attention_mask=input_mask,\n",
    "                    token_type_ids=segment_ids,\n",
    "                    attributes=attributes,\n",
    "                    preconditions=preconditions,\n",
    "                    effects=effects,\n",
    "                    training=True)\n",
    "\n",
    "        out_preconditions_softmax=out_1['out_preconditions_softmax']\n",
    "        out_effects_softmax=out_1['out_effects_softmax']\n",
    "        outcls=out_1['out']\n",
    "        out_2 = conflictModel(input_ids, \n",
    "                    input_lengths,\n",
    "                    input_entities,\n",
    "                    out=outcls,\n",
    "                    attention_mask=input_mask,\n",
    "                    token_type_ids=segment_ids,\n",
    "                    attributes=attributes,\n",
    "                    out_preconditions_softmax=out_preconditions_softmax,\n",
    "                    out_effects_softmax=out_effects_softmax,\n",
    "                    conflicts=conflicts,\n",
    "                    labels=labels,\n",
    "                    training=True)\n",
    "\n",
    "        out={}\n",
    "        for k in out_1:\n",
    "            out[k]=out_1[k]\n",
    "        for k in out_2:\n",
    "            out[k]=out_2[k]\n",
    "        out['total_loss']=ComputeLoss(out_1,out_2)\n",
    "        \n",
    "        loss = out['total_loss']\n",
    "\n",
    "        # Backward pass\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                       1.0)  # Gradient clipping\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Build learning curve data if needed\n",
    "        if build_learning_curves:\n",
    "            train_record = {\n",
    "                'epoch':\n",
    "                len(train_lc_data) - 1,\n",
    "                'iteration':\n",
    "                (len(train_lc_data) - 1) * len(train_dataloader) + step,\n",
    "                'loss_preconditions':\n",
    "                float(out['loss_preconditions'].detach().cpu().numpy()) /\n",
    "                model.num_attributes,\n",
    "                'loss_effects':\n",
    "                float(out['loss_effects'].detach().cpu().numpy()) /\n",
    "                model.num_attributes,\n",
    "                'loss_conflicts':\n",
    "                float(out['loss_conflicts'].detach().cpu().numpy()),\n",
    "                'loss_stories':\n",
    "                float(out['loss_stories'].detach().cpu().numpy()),\n",
    "                'loss_total':\n",
    "                float(out['total_loss'].detach().cpu().numpy())\n",
    "            }\n",
    "            train_lc_data[-1].append(train_record)\n",
    "\n",
    "            # Add a validation record 5 times per epoch\n",
    "            chunk_size = len(train_dataloader) // 5\n",
    "            if (len(train_dataloader) - step - 1) % chunk_size == 0:\n",
    "                validation_results = evaluate_tiered(\n",
    "                    model,\n",
    "                    val_dataloader,\n",
    "                    device, [(accuracy_score, 'accuracy'), (f1_score, 'f1')],\n",
    "                    seg_mode=False,\n",
    "                    return_explanations=True,\n",
    "                    return_losses=True,\n",
    "                    verbose=False)\n",
    "                out = validation_results[16]\n",
    "\n",
    "                val_record = {\n",
    "                    'epoch':\n",
    "                    len(val_lc_data) - 1,\n",
    "                    'iteration':\n",
    "                    (len(val_lc_data) - 1) * len(train_dataloader) + step,\n",
    "                    'loss_preconditions':\n",
    "                    float(out['loss_preconditions'].detach().cpu().numpy()) /\n",
    "                    model.num_attributes,\n",
    "                    'loss_effects':\n",
    "                    float(out['loss_effects'].detach().cpu().numpy()) /\n",
    "                    model.num_attributes,\n",
    "                    'loss_conflicts':\n",
    "                    float(out['loss_conflicts'].detach().cpu().numpy()),\n",
    "                    'loss_stories':\n",
    "                    float(out['loss_stories'].detach().cpu().numpy()),\n",
    "                    'loss_total':\n",
    "                    float(out['total_loss'].detach().cpu().numpy())\n",
    "                }\n",
    "                val_lc_data[-1].append(val_record)\n",
    "\n",
    "        bar_idx += 1\n",
    "        bar.update(bar_idx)\n",
    "\n",
    "    bar.finish()\n",
    "\n",
    "    return total_loss / len(train_dataloader), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65a9271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tiered(MaxStoryLength,tslm_model,trip_model, eval_dataloader, device, metrics, seg_mode=False, return_softmax=False, return_explanations=False, return_losses=False, verbose=True):\n",
    "  if verbose:\n",
    "    print('\\tBeginning evaluation...')\n",
    "\n",
    "  t0 = time.time()\n",
    "\n",
    "  tslm_model.zero_grad()\n",
    "  trip_model.zero_grad()\n",
    "  tslm_model.eval()\n",
    "  trip_model.eval()\n",
    "  for layer in trip_model.precondition_classifiers:\n",
    "    layer.eval()\n",
    "  for layer in trip_model.effect_classifiers:\n",
    "    layer.eval()    \n",
    "\n",
    "  all_pred_attributes = None\n",
    "  all_attributes = None\n",
    "\n",
    "  all_pred_prec = None\n",
    "  all_prec = None\n",
    "\n",
    "  all_pred_eff = None\n",
    "  all_eff = None\n",
    "\n",
    "  all_pred_conflicts = None\n",
    "  all_conflicts = None\n",
    "\n",
    "  all_pred_stories = None\n",
    "  all_stories = None  \n",
    "  if return_softmax:\n",
    "    all_prob_stories = None\n",
    "  \n",
    "  if verbose:\n",
    "    print('\\t\\tRunning prediction...')\n",
    "\n",
    "  if verbose:\n",
    "    bar_size = len(eval_dataloader)\n",
    "    bar = progressbar.ProgressBar(max_value=bar_size, widgets=[progressbar.Bar('#', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar_idx = 0\n",
    "    bar.start()\n",
    "\n",
    "  # Aggregate losses\n",
    "  agg_losses = {}\n",
    "\n",
    "  # Get preds from model\n",
    "  for batch in eval_dataloader:\n",
    "    # Move to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    input_ids = batch[0].long().to(device)\n",
    "    input_lengths = batch[1].to(device)\n",
    "    input_entities = batch[2].to(device)\n",
    "    input_mask = batch[3].to(device)\n",
    "    attributes = batch[4].long().to(device)\n",
    "    preconditions = batch[5].long().to(device)\n",
    "    effects = batch[6].long().to(device)\n",
    "    conflicts = batch[7].long().to(device)\n",
    "    labels = batch[8].long().to(device)\n",
    "    timestep_type_ids=batch[9].long().to(device)\n",
    "    if seg_mode:\n",
    "      segment_ids = batch[9].to(device)\n",
    "    else:\n",
    "      segment_ids = None\n",
    "\n",
    "    batch_size, num_stories, num_entities, num_sents, seq_length = input_ids.shape\n",
    "\n",
    "    with torch.no_grad():\n",
    "            prec_result,effect_result,prec_pred,effect_pred,embedding_result,total_loss_pre,total_loss_effect=\\\n",
    "    tslm_entity_classifier(tslmclassifier,input_ids,input_mask,timestep_type_ids,preconditions,effects,att_to_num_classes,tslm_optimizer)\n",
    "      # out = model(input_ids,\n",
    "\n",
    "    out=trip_model(embedding_result,\n",
    "                   input_ids.shape,\n",
    "                   input_lengths,\n",
    "                   input_entities,\n",
    "                   out_preconditions=prec_pred,\n",
    "                   out_preconditions_softmax=prec_result,\n",
    "                   out_effects=effect_pred,\n",
    "                   out_effects_softmax=effect_result,\n",
    "                   attention_mask=input_mask,\n",
    "                   token_type_ids=segment_ids,\n",
    "                   attributes=attributes,\n",
    "                   preconditions=preconditions,\n",
    "                   effects=effects,\n",
    "                   conflicts=conflicts,\n",
    "                   labels=labels,\n",
    "                   training=True)\n",
    "    \n",
    "        out['loss_preconditions']=total_loss_pre\n",
    "        out['loss_effects']=total_loss_effect\n",
    "        train_loss = out['loss_stories']+out['loss_conflicts']\n",
    "        temp_total_loss=train_loss+total_loss_pre+total_loss_effect\n",
    "        out['total_loss']=temp_total_loss\n",
    "        \n",
    "    if return_losses:\n",
    "      for k in out:\n",
    "        if 'loss' in k:\n",
    "          if k not in agg_losses:\n",
    "            agg_losses[k] = out[k]\n",
    "          else:\n",
    "            agg_losses[k] += out[k]\n",
    "\n",
    "    # Get gt/predicted attributes\n",
    "    if 'attributes' not in trip_model.ablation:\n",
    "      label_ids = attributes.view(-1, attributes.shape[-1]).to('cpu').numpy()\n",
    "      if all_attributes is None:\n",
    "        all_attributes = label_ids\n",
    "      else:\n",
    "        all_attributes = np.concatenate((all_attributes, label_ids), axis=0)\n",
    "\n",
    "      preds = out['out_attributes'].detach().cpu().numpy()\n",
    "      preds[preds >= 0.5] = 1\n",
    "      preds[preds < 0.5] = 0\n",
    "      if all_pred_attributes is None:\n",
    "        all_pred_attributes = preds\n",
    "      else:\n",
    "        all_pred_attributes = np.concatenate((all_pred_attributes, preds), axis=0)\n",
    "\n",
    "\n",
    "    # Get gt/predicted preconditions\n",
    "    label_ids = preconditions.view(-1, preconditions.shape[-1]).to('cpu').numpy()\n",
    "    if all_prec is None:\n",
    "      all_prec = label_ids\n",
    "    else:\n",
    "      all_prec = np.concatenate((all_prec, label_ids), axis=0)\n",
    "\n",
    "    preds = out['out_preconditions'].detach().cpu().numpy()\n",
    "    if all_pred_prec is None:\n",
    "      all_pred_prec = preds\n",
    "    else:\n",
    "      all_pred_prec = np.concatenate((all_pred_prec, preds), axis=0)\n",
    "\n",
    "\n",
    "    # Get gt/predicted preconditions\n",
    "    label_ids = effects.view(-1, effects.shape[-1]).to('cpu').numpy()\n",
    "    if all_eff is None:\n",
    "      all_eff = label_ids\n",
    "    else:\n",
    "      all_eff = np.concatenate((all_eff, label_ids), axis=0)\n",
    "\n",
    "    preds = out['out_effects'].detach().cpu().numpy()\n",
    "    if all_pred_eff is None:\n",
    "      all_pred_eff = preds\n",
    "    else:\n",
    "      all_pred_eff = np.concatenate((all_pred_eff, preds), axis=0)\n",
    "\n",
    "\n",
    "    # Get gt/predicted conflict points\n",
    "    label_ids = conflicts.to('cpu').numpy()\n",
    "    if all_conflicts is None:\n",
    "      all_conflicts = label_ids\n",
    "    else:\n",
    "      all_conflicts = np.concatenate((all_conflicts, label_ids), axis=0)\n",
    "\n",
    "    # preds_start = torch.argmax(out['out_start'],dim=-1).detach().cpu().numpy()\n",
    "    # preds_end = torch.argmax(out['out_end'],dim=-1).detach().cpu().numpy()\n",
    "    # preds = np.stack((preds_start, preds_end), axis=1)\n",
    "\n",
    "    preds = out['out_conflicts'].detach().cpu().numpy()\n",
    "    preds[preds < 0.5] = 0.\n",
    "    preds[preds >= 0.5] = 1.\n",
    "    if all_pred_conflicts is None:\n",
    "      all_pred_conflicts = preds\n",
    "    else:\n",
    "      all_pred_conflicts = np.concatenate((all_pred_conflicts, preds), axis=0)\n",
    "\n",
    "\n",
    "    # Get gt/predicted story choices\n",
    "    label_ids = labels.to('cpu').numpy()\n",
    "    if all_stories is None:\n",
    "      all_stories = label_ids\n",
    "    else:\n",
    "      all_stories = np.concatenate((all_stories, label_ids), axis=0)\n",
    "\n",
    "    preds = torch.argmax(out['out_stories'], dim=-1).detach().cpu().numpy()\n",
    "    if all_pred_stories is None:\n",
    "      all_pred_stories = preds\n",
    "    else:\n",
    "      all_pred_stories = np.concatenate((all_pred_stories, preds), axis=0)\n",
    "    if return_softmax:\n",
    "      probs = torch.softmax(out['out_stories'], dim=-1).detach().cpu().numpy()\n",
    "      if all_prob_stories is None:\n",
    "        all_prob_stories = probs\n",
    "      else:\n",
    "        all_prob_stories = np.concatenate((all_prob_stories, probs), axis=0)\n",
    "\n",
    "    if verbose:\n",
    "      bar_idx += 1\n",
    "      bar.update(bar_idx)\n",
    "  if verbose:\n",
    "    bar.finish()\n",
    "\n",
    "  # Calculate metrics\n",
    "  if verbose:\n",
    "    print('\\t\\tComputing metrics...')\n",
    "\n",
    "  # print(all_pred_attributes.shape)\n",
    "  # print(all_attributes.shape)\n",
    "  # print(all_pred_prec.shape)\n",
    "  # print(all_prec.shape)\n",
    "  # print(all_pred_eff.shape)\n",
    "  # print(all_eff.shape)\n",
    "  # print(all_pred_conflicts.shape)\n",
    "  # print(all_conflicts.shape)\n",
    "  # print(all_pred_stories.shape)\n",
    "  # print(all_stories.shape)\n",
    "\n",
    "  input_lengths = input_lengths.detach().cpu().numpy()\n",
    "\n",
    "  # Overall metrics and per-category metrics for attributes, preconditions, and effects\n",
    "  # NOTE: there are a lot of extra negative examples due to padding along sentene and entity dimenions. This can't affect F1, but will affect accuracy and make it disproportionately large.\n",
    "  metr_attr = None\n",
    "  if 'attributes' not in trip_model.ablation:\n",
    "    metr_attr = compute_metrics(all_pred_attributes.flatten(), all_attributes.flatten(), metrics)\n",
    "    for i in range(trip_model.num_attributes):\n",
    "      metr_i = compute_metrics(all_pred_attributes[:, i], all_attributes[:, i], metrics)\n",
    "      for k in metr_i:\n",
    "        metr_attr['%s_%s' % (str(k), str(i))] = metr_i[k]\n",
    "\n",
    "  metr_prec = compute_metrics(all_pred_prec.flatten(), all_prec.flatten(), metrics)\n",
    "  for i in range(trip_model.num_attributes):\n",
    "    metr_i = compute_metrics(all_pred_prec[:, i], all_prec[:, i], metrics)\n",
    "    for k in metr_i:\n",
    "      metr_prec['%s_%s' % (str(k), str(i))] = metr_i[k]\n",
    "\n",
    "  metr_eff = compute_metrics(all_pred_eff.flatten(), all_eff.flatten(), metrics)\n",
    "  for i in range(trip_model.num_attributes):\n",
    "    metr_i = compute_metrics(all_pred_eff[:, i], all_eff[:, i], metrics)\n",
    "    for k in metr_i:\n",
    "      metr_eff['%s_%s' % (str(k), str(i))] = metr_i[k]\n",
    "\n",
    "  # Conflict span metrics\n",
    "  metr_conflicts = compute_metrics(all_pred_conflicts.flatten(), all_conflicts.flatten(), metrics)\n",
    "\n",
    "  # metr_start = compute_metrics(all_pred_spans[:,0], all_spans[:,0], metrics)\n",
    "  # for k in metr_start:\n",
    "  #   metr[k + '_start'] = metr_start[k]\n",
    "\n",
    "  # metr_end = compute_metrics(all_pred_spans[:,1], all_spans[:,1], metrics)\n",
    "  # for k in metr_end:\n",
    "  #   metr[k + '_end'] = metr_end[k]\n",
    "\n",
    "  metr_stories = compute_metrics(all_pred_stories.flatten(), all_stories.flatten(), metrics)\n",
    "\n",
    "  verifiability, explanations = verifiable_reasoning(all_stories, all_pred_stories, all_conflicts, all_pred_conflicts, all_prec, all_pred_prec, all_eff, all_pred_eff, return_explanations=True)\n",
    "  metr_stories['verifiability'] = verifiability\n",
    "\n",
    "  if verbose:\n",
    "    print('\\tFinished evaluation in %ss.' % str(format_time(time.time() - t0)))\n",
    "\n",
    "  return_base = [metr_attr, all_pred_attributes, all_attributes, metr_prec, all_pred_prec, all_prec, metr_eff, all_pred_eff, all_eff, metr_conflicts, all_pred_conflicts, all_conflicts, metr_stories, all_pred_stories, all_stories]\n",
    "  if return_softmax:\n",
    "    return_base += [all_prob_stories]\n",
    "  if return_explanations:\n",
    "    return_base += [explanations]\n",
    "  if return_losses:\n",
    "    for k in agg_losses:\n",
    "      if 'loss' in k:\n",
    "        agg_losses[k] /= len(eval_dataloader)\n",
    "    return_base += [agg_losses]\n",
    "  \n",
    "  return tuple(return_base)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
