{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb962343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloze label distribution (train):\n",
      "[(1, 400), (0, 399)]\n",
      "Cloze label distribution (dev):\n",
      "[(0, 161), (1, 161)]\n",
      "Cloze label distribution (test):\n",
      "[(1, 176), (0, 175)]\n"
     ]
    }
   ],
   "source": [
    "DRIVE_PATH = ''\n",
    "mode = 'roberta' \n",
    "task_name = 'trip'\n",
    "debug = False\n",
    "config_batch_size = 1\n",
    "config_lr = 1e-6 \n",
    "config_epochs = 15\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "model_name = 'roberta-large'\n",
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-large')\n",
    "from transformers import RobertaConfig,RobertaModel,AdamW\n",
    "config_class = RobertaConfig\n",
    "emb_class = RobertaModel\n",
    "from www.utils import print_dict\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "data_file = os.path.join('../Source_task/trip.json')\n",
    "with open(data_file, 'r') as f:\n",
    "    cloze_dataset_2s, order_dataset_2s = json.load(f)  \n",
    "\n",
    "for p in cloze_dataset_2s:\n",
    "    label_dist = Counter([ex['label'] for ex in cloze_dataset_2s[p]])\n",
    "    print('Cloze label distribution (%s):' % p)\n",
    "    print(label_dist.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443edb99",
   "metadata": {},
   "source": [
    "#### Load trip dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1fd1114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from www.dataset.prepro import get_tiered_data, balance_labels\n",
    "from www.dataset.adaptation_joint import add_bert_features_tiered_modify,get_tensor_dataset_tiered_modify,filter_dataset,getMaxStoryLength,checkdistribution\n",
    "from collections import Counter\n",
    "tiered_dataset = cloze_dataset_2s\n",
    "\n",
    "# Debug the code on a small amount of data\n",
    "debug = True\n",
    "if debug:\n",
    "    for k in tiered_dataset:\n",
    "        tiered_dataset[k] = tiered_dataset[k][:10]\n",
    "\n",
    "maxStoryLength=168       \n",
    "tiered_dataset = get_tiered_data(tiered_dataset)\n",
    "tiered_dataset = add_bert_features_tiered_modify(tiered_dataset, tokenizer,maxStoryLength, add_segment_ids=True)\n",
    "\n",
    "\n",
    "tiered_tensor_dataset = {}\n",
    "max_story_length = max([len(ex['stories'][0]['sentences']) for p in tiered_dataset for ex in tiered_dataset[p]])\n",
    "for p in tiered_dataset:\n",
    "    tiered_tensor_dataset[p] = get_tensor_dataset_tiered_modify(tiered_dataset[p], max_story_length, maxStoryLength,add_segment_ids=True)\n",
    "    \n",
    "    \n",
    "from www.dataset.ann import att_to_idx, att_to_num_classes, att_types\n",
    "\n",
    "subtask = 'cloze'\n",
    "batch_sizes = [config_batch_size]\n",
    "learning_rates = [config_lr]\n",
    "epochs = config_epochs\n",
    "eval_batch_size = 16\n",
    "generate_learning_curve = True # Generate data for training curve figure in TRIP paper\n",
    "\n",
    "num_state_labels = {}\n",
    "for att in att_to_idx:\n",
    "    if att_types[att] == 'default':\n",
    "        num_state_labels[att_to_idx[att]] = 3\n",
    "    else:\n",
    "        num_state_labels[att_to_idx[att]] = att_to_num_classes[att] # Location attributes fall into this since they don't have well-define pre- and post-condition yet\n",
    "\n",
    "ablation = ['attributes', 'states-logits'] # This is the default mode presented in the paper    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056dac04",
   "metadata": {},
   "source": [
    "#### Parameter setting\n",
    "To choose **sentence-centric** set **conflict_pred** to True\n",
    "\n",
    "To choose **entity-centric** set **conflict_pred** to False and **story_label** to False\n",
    "\n",
    "To choose **story-centric** set **conflict_pred** to False and **story_label** to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eb60ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_label=True\n",
    "conflict_pred=True\n",
    "\n",
    "\n",
    "\n",
    "DRIVE_PATH=''\n",
    "story_loss=None \n",
    "story_back=None\n",
    "loss_percent=None\n",
    "if conflict_pred:\n",
    "    story_loss=False\n",
    "    story_back=False\n",
    "    loss_percent = [0.4,0.4,0.2,0]\n",
    "    DRIVE_PATH=\"Single_Model_sentence_centric\"\n",
    "else:\n",
    "    story_loss=True \n",
    "    story_back=True\n",
    "    loss_percent = [0.4,0.4,0.1,0.1]\n",
    "    if story_label:\n",
    "        DRIVE_PATH=\"Single_Model_story_centric\"\n",
    "    else:\n",
    "        DRIVE_PATH=\"Single_Model_entity_centric\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a66464",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e6eb9a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from www.utils import print_dict, get_model_dir\n",
    "# Modify\n",
    "from Method_Single import compute_metrics,update_result,trainModel,trainModel_config,evalModel,evaluation,verifiable_reasoning,save_results\n",
    "from Model_single import RobertaProceduralTSLM\n",
    "from www.dataset.ann import att_to_num_classes\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "seed_val = 24 # Save random seed for reproducibility\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "train_sampler = RandomSampler(tiered_tensor_dataset['train'])\n",
    "train_dataloader = DataLoader(tiered_tensor_dataset['train'], sampler=train_sampler, batch_size=1)\n",
    "dev_sampler = SequentialSampler(tiered_tensor_dataset['dev'])\n",
    "dev_dataloader = DataLoader(tiered_tensor_dataset['dev'], sampler=dev_sampler, batch_size=1)\n",
    "\n",
    "config = config_class.from_pretrained(model_name)  \n",
    "device=\"cuda:0\"\n",
    "num_attributes=len(num_state_labels)\n",
    "max_story_length = max([len(ex['stories'][0]['sentences']) for p in tiered_dataset for ex in tiered_dataset[p]])\n",
    "\n",
    "\n",
    "\n",
    "tslm= RobertaProceduralTSLM.from_pretrained('tli8hf/unqover-roberta-large-squad', return_dict=True,num_attributes=num_attributes,labels_per_att=num_state_labels,story_loss=story_loss,story_back=story_back).to(device)\n",
    "tslm_optimizer = AdamW(tslm.parameters(), lr=config_lr)\n",
    "# total_steps = len(train_dataloader) * epochs\n",
    "# scheduler = get_linear_schedule_with_warmup(tslm_optimizer, num_warmup_steps=0, num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6438f3",
   "metadata": {},
   "source": [
    "#### Train model within source task`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "435a7320",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\nTRAINING MODEL')\n",
    "best_accuracy=0\n",
    "best_verifiability=0\n",
    "best_accuracy_dir=\"\"\n",
    "best_verifiability_dir=\"\"\n",
    "grad_accmu=2\n",
    "for epoch_number in range(epochs):\n",
    "    print(\"Start Training in epoch #{}\".format(epoch_number))\n",
    "    train_lc_data = []\n",
    "    tslm.train()\n",
    "    for layer in tslm.precondition_classifiers:\n",
    "        layer.train()\n",
    "    for layer in tslm.effect_classifiers:\n",
    "        layer.train()    \n",
    "    for batch in tqdm(train_dataloader):\n",
    "        final_out=trainModel_config(batch,max_story_length,maxStoryLength,num_attributes,device\\\n",
    "                             ,tslm,tslm_optimizer,story_label=story_label,grad_accmu=grad_accmu,loss_percent=loss_percent)\n",
    "        train_lc_data.append({'loss_preconditions':float(final_out['loss_preconditions'].detach().cpu().numpy()),\n",
    "                              'loss_effects': float(final_out['loss_effects'].detach().cpu().numpy()),\n",
    "                              'loss_conflicts': float(final_out['loss_conflicts'].detach().cpu().numpy()),\n",
    "                              'loss_stories': float(final_out['loss_story'].detach().cpu().numpy()),\n",
    "                              'loss_total': float(final_out['total_loss'].detach().cpu().numpy())})\n",
    "        \n",
    "    print(\"Start Evaluation in epoch #{}\".format(epoch_number))\n",
    "    tslm.eval()\n",
    "    for layer in tslm.precondition_classifiers:\n",
    "        layer.eval()\n",
    "    for layer in tslm.effect_classifiers:\n",
    "        layer.eval()    \n",
    "    metr_prec,metr_eff,metr_conflicts,metr_stories,verifiability,consistency, explanations=evaluation(max_story_length,maxStoryLength,num_attributes,tslm,tslm_optimizer,dev_dataloader,device,story_label=story_label,conflict_pred=conflict_pred)\n",
    "    accuracy=metr_stories['accuracy']\n",
    "    metr_stories['consistency']=consistency\n",
    "    metr_stories['verifiability']=verifiability\n",
    "    \n",
    "    # save metrics\n",
    "    task_name=\"trip_%s_dev\"\n",
    "    output_model_path=os.path.join(DRIVE_PATH,str(epoch_number))\n",
    "    if not os.path.exists(output_model_path):\n",
    "        os.makedirs(output_model_path)\n",
    "    save_results(metr_prec, output_model_path, task_name % 'preconditions')\n",
    "    save_results(metr_eff, output_model_path, task_name % 'effects')\n",
    "    save_results(metr_conflicts, output_model_path, task_name % 'conflicts')\n",
    "    save_results(metr_stories, output_model_path, task_name % 'stories')\n",
    "    save_results(explanations, output_model_path, task_name % 'explanations')   \n",
    "    train_lc_data = pd.DataFrame(train_lc_data)\n",
    "    train_lc_data.to_csv(os.path.join(output_model_path, 'learning_curve_data_train.csv'), index=False)\n",
    "\n",
    "    \n",
    "    model_dir=os.path.join(output_model_path, 'tslm.pth')\n",
    "    torch.save(tslm, model_dir)\n",
    "    \n",
    "    if best_accuracy < accuracy:\n",
    "        best_accuracy=accuracy\n",
    "        best_accuracy_dir=model_dir\n",
    "    if best_verifiability < verifiability:\n",
    "        best_verifiability=verifiability\n",
    "        best_verifiability_dir=model_dir\n",
    "    print(\"accuracy {}\".format(accuracy))\n",
    "    print(\"consistency {}\".format(consistency))\n",
    "    print(\"verifiability {}\".format(verifiability))\n",
    "print(\"Traing Process Finish\")\n",
    "print(\"Achieve best accuracy {} at {}\".format(best_accuracy,best_accuracy_dir))\n",
    "print(\"Achieve best verifiability {} at {}\".format(best_verifiability,best_verifiability_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f1d1cb",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25380933",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in tiered_dataset:\n",
    "    if p != 'test':\n",
    "        continue\n",
    "\n",
    "    p_dataset = tiered_dataset[p]\n",
    "    p_tensor_dataset = tiered_tensor_dataset[p]\n",
    "    p_sampler = SequentialSampler(p_tensor_dataset)\n",
    "    p_dataloader = DataLoader(p_tensor_dataset,\n",
    "                              sampler=p_sampler,\n",
    "                              batch_size=1)\n",
    "    task_name = 'trip' + '_%s_' + p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c149b6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dir_file=DRIVE_PATH\n",
    "for i in range(15):\n",
    "    file_number=str(i)\n",
    "    output_model_path=os.path.join(Dir_file, file_number)\n",
    "    tslm = torch.load(os.path.join(output_model_path, 'tslm.pth'))\n",
    "    print(\"Start Evaluation in epoch #{}\".format(i))\n",
    "    tslm.eval()\n",
    "    metr_prec,metr_eff,metr_conflicts,metr_stories,verifiability,consistency, explanations=evaluation(max_story_length,maxStoryLength,num_attributes,tslm,tslm_optimizer,p_dataloader,device,story_label=story_label,conflict_pred=conflict_pred)\n",
    "    accuracy=metr_stories['accuracy']\n",
    "    metr_stories['consistency']=consistency\n",
    "    metr_stories['verifiability']=verifiability\n",
    "    print(\"accuracy {}\".format(accuracy))\n",
    "    print(\"consistency {}\".format(consistency))\n",
    "    print(\"verifiability {}\".format(verifiability))\n",
    "    save_results(metr_prec, output_model_path, task_name % 'preconditions')\n",
    "    save_results(metr_eff, output_model_path, task_name % 'effects')\n",
    "    save_results(metr_conflicts, output_model_path, task_name % 'conflicts')\n",
    "    save_results(metr_stories, output_model_path, task_name % 'stories')\n",
    "    save_results(explanations, output_model_path, task_name % 'explanations') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93c5d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
