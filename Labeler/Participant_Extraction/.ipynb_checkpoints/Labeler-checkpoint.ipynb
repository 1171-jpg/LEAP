{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1caf3500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import pickle as pk\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.etree.ElementTree import Element\n",
    "from xml.etree.ElementTree import SubElement\n",
    "from xml.etree.ElementTree import ElementTree\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader import Synset\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.lang.en import English\n",
    "nlp_sen = English()\n",
    "config = {\"punct_chars\": None}\n",
    "nlp_sen.add_pipe(\"sentencizer\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f69229e",
   "metadata": {},
   "source": [
    "### Pre setting of ESC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f191db5",
   "metadata": {},
   "source": [
    "#### download ESC model published at the official github(https://github.com/SapienzaNLP/esc)  via link https://drive.google.com/file/d/100jxjLIdmSzrMXXOWgrPz93EG0JBnkfr/view and put the ckpt file in ESC_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e92fabe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "from typing import NamedTuple, List, Optional, Tuple\n",
    "\n",
    "os.environ['CUDA_PATH']='/usr/local/cuda'\n",
    "os.environ['LD_LIBRARY_PATH']='$CUDA_PATH/lib64:$LD_LIBRARY_PATH'\n",
    "ROOT_DIR = os.path.abspath('./')\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from esc.utils.definitions_tokenizer import get_tokenizer\n",
    "from esc.utils.wordnet import synset_from_offset\n",
    "from esc.utils.wsd import WSDInstance\n",
    "from esc.esc_dataset import WordNetDataset, OxfordDictionaryDataset\n",
    "from esc.esc_pl_module import ESCModule\n",
    "from esc.utils.commons import list_elems_in_dir\n",
    "from esc.predict import InstancePredictionReport,ScoresReport,PredictionReport,probabilistic_prediction,precision_recall_f1_accuracy_score,predict,process_prediction_result\n",
    "\n",
    "import ntpath\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e83cb0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d716a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ntpath\n",
    "import logging\n",
    "\n",
    "### parameter and model\n",
    "args={}\n",
    "args['ckpt'] = './ESC_Model/escher_semcor_best.ckpt'\n",
    "args['dataset_paths'] = 'Test.data.xml'\n",
    "args['prediction_types'] = 'probabilistic'\n",
    "args['evaluate'] = False\n",
    "args['device'] = 0\n",
    "args['tokens_per_batch'] = 4000\n",
    "args['output_errors'] = False\n",
    "args['oxford_test'] = False\n",
    "wsd_model = ESCModule.load_from_checkpoint(args['ckpt'])\n",
    "wsd_model.freeze()\n",
    "\n",
    "tokenizer = get_tokenizer(\n",
    "    wsd_model.hparams.transformer_model, getattr(wsd_model.hparams, \"use_special_tokens\", False)\n",
    ")\n",
    "\n",
    "prediction_type = args['prediction_types']\n",
    "\n",
    "dataset_path = args['dataset_paths']\n",
    "dataset_path = dataset_path.replace(\".data.xml\", \"\").replace(\".gold.key.txt\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97ba202",
   "metadata": {},
   "source": [
    "### Presetting of Extration\n",
    "\n",
    "Set **debug** to True when test *Labeler* on small version of data.  \n",
    "\n",
    "Initial state of **input_file** is False, after first time running, all the xml file will be stored locally. Set **input_file** to True to use the stored xml file directly.  \n",
    "\n",
    "**Log** file contains running information during the training.  \n",
    "\n",
    "**Participant** file store the final result of participant extration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "947a5643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from participant import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a57285d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New folder created\n",
      "New folder created\n"
     ]
    }
   ],
   "source": [
    "debug = True\n",
    "log_file_name = \"./Log\"\n",
    "mkdir(log_file_name)\n",
    "final_file_name = \"./Final_Participant\"\n",
    "mkdir(final_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3555b1cb",
   "metadata": {},
   "source": [
    "### CODAH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a10a0ba",
   "metadata": {},
   "source": [
    "Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "740a1c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2776/2776 [00:01<00:00, 2338.01it/s]\n"
     ]
    }
   ],
   "source": [
    "file_name = '../../Target_task/codah.tsv'\n",
    "dev_dataset = get_codah_dataset(file_name)\n",
    "\n",
    "if debug:\n",
    "    dev_dataset = dev_dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaa576f",
   "metadata": {},
   "source": [
    "Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63586ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_file_name = \"./XML/CODAH\"\n",
    "mkdir(xml_file_name)\n",
    "input_file=False\n",
    "for data_index,sample_data in tqdm(enumerate(dev_dataset)):\n",
    "        try:\n",
    "            with open(\"./Log/CODAH_LOG.txt\",\"a\") as file:\n",
    "                file.write(\"prompting begin at {data_index}\\n\".format(data_index=data_index))\n",
    "\n",
    "            sen_idx = 0\n",
    "            sen = sample_data['sentences'][0][0]\n",
    "            sol_idx = -1\n",
    "            file_name = os.path.join(xml_file_name, \"{data_index}_{sen_idx}_{sol_idx}.xml\".format(data_index=data_index,sen_idx=sen_idx,sol_idx=sol_idx))\n",
    "            pre_entity = list(GetEntityList(nlp,sen,file_name=file_name,input_file=input_file))\n",
    "            entity_list = [[],[],[],[]]\n",
    "            ### end 1\n",
    "            for sol_idx,sen_list in enumerate(sample_data['sentences']):\n",
    "                for sen_idx,sen in enumerate(sen_list):\n",
    "                    if sen_idx>0:\n",
    "                        file_name =os.path.join(xml_file_name, \"{data_index}_{sen_idx}_{sol_idx}.xml\".format(data_index=data_index,sen_idx=sen_idx,sol_idx=sol_idx))\n",
    "                        entity_list[sol_idx] +=  list(GetEntityList(nlp,sen,file_name=file_name,input_file=input_file)) \n",
    "            for index in range(len(entity_list)):\n",
    "                entity_list[index] += pre_entity\n",
    "            sample_data['entity'] = entity_list\n",
    "        except:\n",
    "            with open(\"./Log/CODAH_LOG.txt\",\"a\") as file:\n",
    "                file.write(\"wrong at {data_index}\\n\".format(data_index=data_index))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f02101",
   "metadata": {},
   "source": [
    "Prepocess and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db140a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 47.44it/s]\n",
      "5it [00:00, 33026.02it/s]\n",
      "5it [00:00, 1246.67it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_dataset = codah_post_preprocess(dev_dataset)\n",
    "np.save(\"Final_Participant/CODAH_Participant\",dev_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be52a62b",
   "metadata": {},
   "source": [
    "### ROCStories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca90f6",
   "metadata": {},
   "source": [
    "Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebf11969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1872it [00:00, 120176.58it/s]\n"
     ]
    }
   ],
   "source": [
    "file_name = \"../../Target_task/rocstories.csv\"\n",
    "dev_dataset = get_roc_dataset(file_name)\n",
    "\n",
    "if debug:\n",
    "    dev_dataset = dev_dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087eee8a",
   "metadata": {},
   "source": [
    "Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1380e668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New folder created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:18, 18.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No synsets found in WordNet for instance d000.s000.t000. Skipping this instance\n",
      "WSDInstance(annotated_token=AnnotatedToken(text='them', pos='NOUN', lemma='them'), labels=None, instance_id='d000.s000.t000')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:21, 16.32s/it]\n"
     ]
    }
   ],
   "source": [
    "xml_file_name = \"./XML/ROC\"\n",
    "mkdir(xml_file_name)\n",
    "input_file=False\n",
    "for data_index,sample_data in tqdm(enumerate(dev_dataset)):\n",
    "    try:\n",
    "\n",
    "        with open(\"./Log/ROC_LOG.txt\",\"a\") as file:\n",
    "            file.write(\"prompting begin at {data_index}\\n\".format(data_index=data_index))\n",
    "        entity_1 = set()\n",
    "        entity_2 = set()\n",
    "        for sen_idx,sen in enumerate(sample_data['sentence']):\n",
    "            sol_idx=0\n",
    "            file_name = os.path.join(xml_file_name, \"{data_index}_{sen_idx}_{sol_idx}.xml\".format(data_index=data_index,sen_idx=sen_idx,sol_idx=sol_idx))\n",
    "            pre_entity = set(GetEntityList(nlp,sen,file_name=file_name,input_file=input_file))\n",
    "            entity_1 = entity_1 | pre_entity\n",
    "            entity_2 = entity_2 | pre_entity\n",
    "        ### end 1\n",
    "        sen = sample_data['end1']\n",
    "        sen_idx=0\n",
    "        sol_idx=1\n",
    "        file_name =os.path.join(xml_file_name, \"{data_index}_{sen_idx}_{sol_idx}.xml\".format(data_index=data_index,sen_idx=sen_idx,sol_idx=sol_idx))\n",
    "        entity_1 = entity_1 | set(GetEntityList(nlp,sen,file_name=file_name,input_file=input_file))    \n",
    "        ### end 2\n",
    "        sen = sample_data['end2']\n",
    "        sen_idx=0\n",
    "        sol_idx=2\n",
    "        file_name =os.path.join(xml_file_name, \"{data_index}_{sen_idx}_{sol_idx}.xml\".format(data_index=data_index,sen_idx=sen_idx,sol_idx=sol_idx))\n",
    "        entity_2 = entity_2 | set(GetEntityList(nlp,sen,file_name=file_name,input_file=input_file))   \n",
    "        sample_data['entity_1']=entity_1\n",
    "        sample_data['entity_2']=entity_2\n",
    "    except:\n",
    "        with open(\"./Log/ROC_LOG.txt\",\"a\") as file:\n",
    "            file.write(\"wrong at {data_index}\\n\".format(data_index=data_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8880bc",
   "metadata": {},
   "source": [
    "Prepocess and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14e3b5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 65.97it/s]\n",
      "5it [00:00, 14614.30it/s]\n",
      "5it [00:00, 753.34it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_dataset  = post_preprocess(dev_dataset)\n",
    "np.save(\"./Final_Participant/ROC_Participant\",dev_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c66a08",
   "metadata": {},
   "source": [
    "### PIQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc5d439",
   "metadata": {},
   "source": [
    "Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12490e11",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1838/1838 [00:00<00:00, 3392.62it/s]\n"
     ]
    }
   ],
   "source": [
    "input_name = '../../Target_task/piqa.jsonl'\n",
    "label_name = '../../Target_task/piqa-labels.lst'\n",
    "dev_dataset = get_piqa_dataset(input_name,label_name,nlp,nlp_sen)\n",
    "\n",
    "if debug:\n",
    "    dev_dataset = dev_dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d18a77",
   "metadata": {},
   "source": [
    "Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14d4a89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_file_name = \"./XML/PIQA\"\n",
    "mkdir(xml_file_name)\n",
    "\n",
    "input_file=False\n",
    "for data_index,sample_data in tqdm(enumerate(dev_dataset[:5])):\n",
    "    try:\n",
    "        with open(\"./Log/PIQA_LOG.txt\",\"a\") as file:\n",
    "            file.write(\"prompting begin at {data_index}\\n\".format(data_index=data_index))\n",
    "        entity_1 = set()\n",
    "        entity_2 = set()\n",
    "        for sen_idx,sen in enumerate(sample_data['goal_sol_1']):\n",
    "            sol_idx=1\n",
    "            file_name = os.path.join(xml_file_name, \"{data_index}_{sen_idx}_{sol_idx}.xml\".format(data_index=data_index,sen_idx=sen_idx,sol_idx=sol_idx))\n",
    "            entity_1 = entity_1 | set(GetEntityList(nlp,sen,file_name=file_name,input_file=input_file))\n",
    "        for sen_idx,sen in enumerate(sample_data['goal_sol_2']):\n",
    "            sol_idx=2\n",
    "            file_name =os.path.join(xml_file_name, \"{data_index}_{sen_idx}_{sol_idx}.xml\".format(data_index=data_index,sen_idx=sen_idx,sol_idx=sol_idx))\n",
    "            entity_2 = entity_2 | set(GetEntityList(nlp,sen,file_name=file_name,input_file=input_file))    \n",
    "\n",
    "        sample_data['entity_1']=entity_1\n",
    "        sample_data['entity_2']=entity_2\n",
    "    except:\n",
    "        with open(\"./Log/PIQA_LOG.txt\",\"a\") as file:\n",
    "            file.write(\"wrong at {data_index}\\n\".format(data_index=data_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023caf0d",
   "metadata": {},
   "source": [
    "Prepocess and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7f0d645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 60787.01it/s]\n",
      "5it [00:00, 36345.79it/s]\n",
      "5it [00:00, 538.42it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_dataset  = post_preprocess(dev_dataset)\n",
    "np.save(\"./Final_Participant/PIQA_Participant\",dev_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec21630",
   "metadata": {},
   "source": [
    "### ANLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1179a4e9",
   "metadata": {},
   "source": [
    "Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "309f39cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1532/1532 [00:00<00:00, 2976.75it/s]\n"
     ]
    }
   ],
   "source": [
    "input_name = '../../Target_task/anli.jsonl'\n",
    "label_name = '../../Target_task/anli-labels.lst'\n",
    "dev_dataset = get_anli_dataset(input_name,label_name,nlp,nlp_sen)\n",
    "\n",
    "if debug:\n",
    "    dev_dataset = dev_dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4fee76",
   "metadata": {},
   "source": [
    "Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3644a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New folder created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:43,  8.65s/it]\n"
     ]
    }
   ],
   "source": [
    "xml_file_name = \"./XML/ANLI\"\n",
    "mkdir(xml_file_name)\n",
    "input_file=False\n",
    "for data_index,sample_data in tqdm(enumerate(dev_dataset)):\n",
    "    try:\n",
    "        with open(\"./Log/ANLI_LOG_sup.txt\",\"a\") as file:\n",
    "            file.write(\"prompting begin at {data_index}\\n\".format(data_index=data_index))\n",
    "        entity_1 = set()\n",
    "        entity_2 = set()\n",
    "        for sen_idx,sen in enumerate([sample_data['obs1'],sample_data['obs2']]):\n",
    "            sol_idx = 0\n",
    "            file_name = os.path.join(xml_file_name, \"{data_index}_{sen_idx}_{sol_idx}.xml\".format(data_index=data_index,sen_idx=sen_idx,sol_idx=sol_idx))\n",
    "            pre_entity = set(GetEntityList(nlp,sen,file_name=file_name,input_file=input_file))\n",
    "            entity_1 = entity_1 | pre_entity\n",
    "            entity_2 = entity_2 | pre_entity\n",
    "\n",
    "        # hpyo_1\n",
    "        sen = sample_data['hyp1']\n",
    "        sen_idx=0\n",
    "        sol_idx=1   \n",
    "        file_name =os.path.join(xml_file_name, \"{data_index}_{sen_idx}_{sol_idx}.xml\".format(data_index=data_index,sen_idx=sen_idx,sol_idx=sol_idx))\n",
    "        entity_1 = entity_1 | set(GetEntityList(nlp,sen,file_name=file_name,input_file=input_file))  \n",
    "\n",
    "        # hpyo_2\n",
    "        sen = sample_data['hyp2']\n",
    "        sen_idx=0\n",
    "        sol_idx=2   \n",
    "        file_name =os.path.join(xml_file_name, \"{data_index}_{sen_idx}_{sol_idx}.xml\".format(data_index=data_index,sen_idx=sen_idx,sol_idx=sol_idx))\n",
    "        entity_2 = entity_2 | set(GetEntityList(nlp,sen,file_name=file_name,input_file=input_file))  \n",
    "\n",
    "        sample_data['entity_1']=entity_1\n",
    "        sample_data['entity_2']=entity_2\n",
    "    except:\n",
    "        with open(\"./Log/ANLI_LOG_sup.txt\",\"a\") as file:\n",
    "            file.write(\"wrong at {data_index}\\n\".format(data_index=data_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528d25c1",
   "metadata": {},
   "source": [
    "Prepocess and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57a1fa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 30750.03it/s]\n",
      "5it [00:00, 47127.01it/s]\n",
      "5it [00:00, 944.79it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_dataset  = post_preprocess(dev_dataset)\n",
    "np.save(\"./Final_Participant/ANLI_Participant\",dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e0b61d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
