{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1ac0a02",
   "metadata": {},
   "source": [
    "### Notice, running this code need a codex api request from openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a7bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import string\n",
    "from utils import *\n",
    "from www.dataset.ann import *\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#openai.api_key = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4e45a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f0792c",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85fe830",
   "metadata": {},
   "source": [
    "#### 1) get Tripdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aec5a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attribute import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82209921",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = ('../../Source_task/trip.json')\n",
    "with open(data_file, 'r') as f:\n",
    "    cloze_dataset_2s, order_dataset_2s = json.load(f)  \n",
    "\n",
    "for p in cloze_dataset_2s:\n",
    "    label_dist = Counter([ex['label'] for ex in cloze_dataset_2s[p]])\n",
    "    print('Cloze label distribution (%s):' % p)\n",
    "    print(label_dist.most_common())\n",
    "\n",
    "tiered_dataset = cloze_dataset_2s\n",
    "\n",
    "\n",
    "maxStoryLength=168       \n",
    "tiered_dataset = get_tiered_data(tiered_dataset)\n",
    "tiered_dataset = add_bert_features_tiered_modify(tiered_dataset, robertatokenizer,maxStoryLength, add_segment_ids=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd4ac0b",
   "metadata": {},
   "source": [
    "## Presetting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "666f6825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KATE(Sentence,Entity,human_flag = False):\n",
    "    for name in Name_list:\n",
    "        Sentence = Sentence.replace(name,\"Someone\")\n",
    "        \n",
    "    Target_Sentence = Sentence+\" Focus: \"+Entity+\".\"\n",
    "    target_output = SentenceTransformer.encode(Target_Sentence,output_value = None)\n",
    "    target_sentenceemb = target_output['sentence_embedding'].cpu().numpy()\n",
    "    target_wordemb = get_wordemb(target_output)\n",
    "    if human_flag:\n",
    "        for key in attribute_sen.keys():\n",
    "            for sen_output in attribute_sen[key]:\n",
    "                sentence =sen_output['sentence']\n",
    "                original_sen = sentence.split(\".\")[0]\n",
    "                entity_name = sentence.split(\":\")[-1].split('.')[0]\n",
    "                sen_emb = sen_output['sentence_embedding']\n",
    "                word_emb = sen_output['word_embeddings']\n",
    "                sen_cos = computeCos(target_sentenceemb,sen_emb)\n",
    "                word_cos = computeCos(target_wordemb,word_emb)\n",
    "                sen_output['Cos_similarity'] = 0.6*sen_cos+0.4*word_cos\n",
    "    else:\n",
    "        for key in attribute_sen.keys():\n",
    "            for sen_output in attribute_sen[key]:\n",
    "                sentence =sen_output['sentence']\n",
    "                original_sen = sentence.split(\".\")[0]\n",
    "                entity_name = sentence.split(\":\")[-1].split('.')[0]\n",
    "                if entity_name in original_sen:\n",
    "                    sen_emb = sen_output['sentence_embedding']\n",
    "                    word_emb = sen_output['word_embeddings']\n",
    "                    sen_cos = computeCos(target_sentenceemb,sen_emb)\n",
    "                    word_cos = computeCos(target_wordemb,word_emb)\n",
    "                    sen_output['Cos_similarity'] = 0.6*sen_cos+0.4*word_cos\n",
    "                    \n",
    "                    \n",
    "    distance_list  = []      \n",
    "    min_length = min([len(attribute_sen[key]) for key in attribute_sen.keys()])\n",
    "    min_length = min(4,min_length)\n",
    "    for key in attribute_sen.keys():\n",
    "        temp_list = attribute_sen[key]\n",
    "        temp_list = sorted(temp_list, key = lambda i : (-i['Cos_similarity']))\n",
    "        for i in range(min_length):\n",
    "            temp_pro = temp_list[i]\n",
    "            temp_pro['attribute_label'] = int(key)\n",
    "            distance_list.append(temp_pro)\n",
    "\n",
    "    \n",
    "    return distance_list\n",
    "\n",
    "\n",
    "def singleAttributePrompting(sentence,entity,human_flag,output_log):\n",
    "    local_count=30\n",
    "#     print(sentence,entity)\n",
    "    while True:\n",
    "        try:\n",
    "            \n",
    "            \n",
    "            prompting_list  = KATE(sentence,entity,human_flag)\n",
    "            score_list = [_['Cos_similarity'] for _ in prompting_list]\n",
    "            key_list = set([_['attribute_label'] for _ in prompting_list])\n",
    "            score_list = np.array(score_list)\n",
    "            total_length = len(key_list)\n",
    "            max_value = np.max(score_list) \n",
    "            max_idx = np.where(score_list == max_value)[0][0]\n",
    "            label = None\n",
    "            if max_value > 0.95:\n",
    "                label = int(max_idx/total_length)   \n",
    "                return label\n",
    "            else:\n",
    "                sentence_prompting = \"\"\n",
    "                for i in range(len(prompting_list)):\n",
    "                    prompinting_temp = prompting_list[i]\n",
    "                    original_sentence = prompinting_temp['sentence']\n",
    "                    original_sen = original_sentence.split(\".\")[0]+\".\"\n",
    "                    entity_name = original_sentence.split(\":\")[-1].split('.')[0]\n",
    "                    attribute_label = prompinting_temp['attribute_label']\n",
    "                    sentence_prompting += getSingleprompt(attribute,(original_sen,entity_name),attribute_label)+'\\n\\n'\n",
    "\n",
    "\n",
    "                input_prompting_entity = getSingleprompt(attribute,(sentence,entity),'_',input_text=True)\n",
    "                input_prompting_entity = sentence_prompting + input_prompting_entity\n",
    "    #             print(input_prompting_entity+'\\n\\n\\n')\n",
    "                response = openai.Completion.create(\n",
    "                  model=\"code-davinci-002\",\n",
    "                  prompt= input_prompting_entity,\n",
    "                  temperature=0,\n",
    "                  max_tokens=1000,\n",
    "                  top_p=1,\n",
    "                  frequency_penalty=0,\n",
    "                  presence_penalty=0,\n",
    "                  stop = '#END'\n",
    "                )\n",
    "\n",
    "                response_clean = response['choices'][0]['text']\n",
    "                activate_label = int(response_clean.split('=')[1].split('\\n')[0].strip())\n",
    "                if total_length ==9:\n",
    "                    if activate_label>8:\n",
    "                        activate_label =8\n",
    "                if total_length == 3:\n",
    "                    if activate_label>2:\n",
    "                        activate_label =2                    \n",
    "                return int(activate_label)\n",
    "\n",
    "        except Exception as e:\n",
    "            time.sleep(local_count)\n",
    "            print(\"error! wait {local_count}s\".format(local_count=local_count))\n",
    "            with open(output_log,\"a\") as file:\n",
    "                file.write(\"error! wait {local_count}s\\n\".format(local_count=local_count))\n",
    "#                 file.write(str(e))\n",
    "            continue\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc73acb",
   "metadata": {},
   "source": [
    "### CSKG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec895aa8",
   "metadata": {},
   "source": [
    "loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca2bde9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Name_list = np.load('Name_list.npy',allow_pickle = True)\n",
    "cskg_data = np.load('../../Source_task/cskg_data.npy',allow_pickle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab36a3b",
   "metadata": {},
   "source": [
    "main pipepline of prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c6a4fd4",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def PromptingPipe_CSKG(dev_dataset,attribute,activate_list,attribute_label_list,log_name,human_flag = False):\n",
    "    attribute_id = att_to_idx[attribute]\n",
    "    Pred_summary = []\n",
    "    count = 0\n",
    "    output_log = os.path.join(log_name,\"{}_log.txt\".format(attribute))\n",
    "#     os.remove(\"./codex_log/{attribute}_log.txt\".format(attribute=attribute))\n",
    "    with tqdm(total=len(activate_list)) as pbar:\n",
    "        while True:\n",
    "            temp_pred_label ={}\n",
    "            try:\n",
    "                with open(output_log,\"a\") as file:\n",
    "                    file.write(\"prompting begin at {count}\\n\".format(count=count))\n",
    "                    \n",
    "                test_story = dev_dataset[activate_list[count]]\n",
    "\n",
    "                    ###\n",
    "                for story_index in range(1,3):\n",
    "                    \n",
    "                    story_key = \"story{story_index}\".format(story_index=story_index)\n",
    "                    entity_key = \"entity_{story_index}\".format(story_index=story_index)\n",
    "\n",
    "                    entity_list = test_story[entity_key]\n",
    "                    sentence_list = test_story[story_key]\n",
    "                    InputPrompting = ''\n",
    "                    InputPrompting = Prompting +getStoryPrompting_CSKG(sentence_list,entity_list)\n",
    "                    Pred_label = np.zeros([len(entity_list),3])\n",
    "\n",
    "                    response = openai.Completion.create(\n",
    "                      model=\"code-davinci-002\",\n",
    "                      prompt= InputPrompting,\n",
    "                      temperature=0,\n",
    "                      max_tokens=1000,\n",
    "                      top_p=1,\n",
    "                      frequency_penalty=0,\n",
    "                      presence_penalty=0,\n",
    "                      stop = '#END'\n",
    "                    )\n",
    "\n",
    "                    response_clean = response['choices'][0]['text'].split(':')[1:]\n",
    "\n",
    "                    for sentence_index,sub_response in enumerate(response_clean):\n",
    "                        sub_response = sub_response.split(\"\\n\\t\\t\")[1:]\n",
    "                        sub_label = []\n",
    "                        if 'pass\\n' in sub_response[0]:\n",
    "                            continue\n",
    "                        else:\n",
    "                            for entity_index,state_response in enumerate(sub_response):\n",
    "                                if entity_index == len(sub_response)-1:  ### the last item in slash n\n",
    "                                    state_response = state_response.split(\"\\n\")[0]\n",
    "                                entity_name = state_response.split('.')[0].strip()\n",
    "                                if human_flag:\n",
    "                                    if entity_name not in Name_list:\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        entity_attribute = state_response.split('.')[1]\n",
    "                                        if entity_attribute==attribute and entity_name in entity_list:\n",
    "                                            sentence = sentence_list[sentence_index]\n",
    "                                            attribute_label = singleAttributePrompting(sentence,entity_name,human_flag,output_log)\n",
    "                                            Pred_label[entity_list.index(entity_name)][sentence_index] = attribute_label\n",
    "                                else:\n",
    "                                    if entity_name in Name_list:\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        entity_attribute = state_response.split('.')[1]\n",
    "                                        if entity_attribute==attribute and entity_name in entity_list:\n",
    "                                            sentence = sentence_list[sentence_index]\n",
    "                                            attribute_label = singleAttributePrompting(sentence,entity_name,human_flag,output_log)\n",
    "                                            Pred_label[entity_list.index(entity_name)][sentence_index] = attribute_label\n",
    "                                        \n",
    "\n",
    "                    ###\n",
    "                    temp_pred_label[story_index] = Pred_label\n",
    "                count +=1\n",
    "                pbar.update(1)\n",
    "                for key,value in temp_pred_label.items():\n",
    "                    Pred_summary.append(value)\n",
    "                if count == len(activate_list):\n",
    "                    break\n",
    "\n",
    "            except Exception as e:\n",
    "                with open(output_log.format(attribute=attribute),\"a\") as file:\n",
    "                    file.write(\"reach limitation at {count}\\n\".format(count=count))\n",
    "                    file.write(\"{e}\\n\".format(e=e))\n",
    "                print(\"reach limitation at {count}\".format(count=count))\n",
    "                time.sleep(30)\n",
    "                continue\n",
    "                \n",
    "    return Pred_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcb5c46",
   "metadata": {},
   "source": [
    "We use human attribute **conscious** as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "652ee627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder exist\n",
      "Folder exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:26<00:26, 26.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error! wait 30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:31<00:00, 45.59s/it]\n"
     ]
    }
   ],
   "source": [
    "summary_dict={}\n",
    "folder_path = 'CSKG'\n",
    "state='effects'\n",
    "file_name= \"{}_{}\".format(folder_path,state)\n",
    "log_name=\"{}_{}_log\".format(folder_path,state)\n",
    "mkdir(file_name)\n",
    "mkdir(log_name)\n",
    "human_flag = False\n",
    "for attribute in ['conscious']:\n",
    "    \n",
    "# for attribute in ['clean']:\n",
    "    attribute_label_list = list(att_adj[attribute])\n",
    "    attribute_id = att_to_idx[attribute]\n",
    "    if attribute_id < 5:\n",
    "        human_flag = True\n",
    "    activate_list = np.arange(len(cskg_data))[:2]\n",
    "#     attribute_sen = get_attribute_dataset(tiered_dataset,Name_list,attribute_id,state)\n",
    "\n",
    "\n",
    "\n",
    "    if attribute == 'location':\n",
    "        Prompting = getProProming(attribute,'_',2,tiered_dataset,state)\n",
    "    else:\n",
    "        Prompting = getProProming(attribute,'_',6,tiered_dataset,state)\n",
    "    \n",
    "    \n",
    "    Pred_summary=PromptingPipe_CSKG(cskg_data,attribute,activate_list,attribute_label_list,log_name,human_flag)\n",
    "\n",
    "    folder_path = 'CSKG'\n",
    "    output_file = os.path.join(file_name,attribute+\".npy\")\n",
    "    np.save(output_file,Pred_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33593504",
   "metadata": {},
   "source": [
    "### ROCStories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f3b46d",
   "metadata": {},
   "source": [
    "loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3362400",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_data = np.load('../Participant_Extraction/Participant/ROC_Participant.npy',allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fa4153e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in roc_data:\n",
    "    list1=sorted(sample['entity_1'])\n",
    "    list2=sorted(sample['entity_2'])\n",
    "    common_list=set(list1)&set(list2)\n",
    "    common_list=sorted(list(common_list))\n",
    "    sample['common_entity']=len(common_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc50931f",
   "metadata": {},
   "source": [
    "main pipeline for prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55cb17bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PromptingPipe_ROC(roc_data,attribute,activate_list,attribute_label_list,log_name,human_flag = False):\n",
    "    attribute_id = att_to_idx[attribute]\n",
    "    Pred_summary = []\n",
    "    count = 0\n",
    "    output_log = os.path.join(log_name,\"{}_log.txt\".format(attribute))\n",
    "    with tqdm(total=len(activate_list)) as pbar:\n",
    "        while True:\n",
    "            temp_pred_label ={}\n",
    "            with open(output_log,\"a\") as file:\n",
    "                file.write(\"prompting begin at {count}\\n\".format(count=count))\n",
    "\n",
    "            test_story = roc_data[activate_list[count]]\n",
    "            if len(test_story['entity_1'])*len(test_story['entity_2'])==0:\n",
    "                with open(output_log,\"a\") as file:\n",
    "                    file.write(\"skip at {count}\\n\".format(count=count))                    \n",
    "                count +=1\n",
    "                pbar.update(1)\n",
    "                Pred_label = np.zeros([len(entity_list),5])\n",
    "                Pred_summary.append(Pred_label)\n",
    "                Pred_summary.append(Pred_label)\n",
    "                if count == len(activate_list):\n",
    "                    break\n",
    "                continue\n",
    "\n",
    "            try:                    \n",
    "                for story_index in range(1,3):\n",
    "                    \n",
    "                    story_key = \"goal_sol_{story_index}\".format(story_index=story_index)\n",
    "                    entity_key = \"entity_{story_index}\".format(story_index=story_index)\n",
    "                    common_entity = test_story['common_entity']\n",
    "                    entity_list = list(test_story[entity_key])\n",
    "                    sentence_list = test_story[story_key]\n",
    "                    InputPrompting = ''\n",
    "                    InputPrompting = Prompting +getStoryPrompting_CSKG(sentence_list,entity_list)\n",
    "                    Pred_label = np.zeros([len(entity_list),5])\n",
    "                    if story_index == 2 and common_entity >0:\n",
    "                        Pred_label[:common_entity,:4] = temp_pred_label[1][:common_entity,:4]\n",
    "\n",
    "                    response = openai.Completion.create(\n",
    "                      model=\"code-davinci-002\",\n",
    "                      prompt= InputPrompting,\n",
    "                      temperature=0,\n",
    "                      max_tokens=1000,\n",
    "                      top_p=1,\n",
    "                      frequency_penalty=0,\n",
    "                      presence_penalty=0,\n",
    "                      stop = '#END'\n",
    "                    )\n",
    "\n",
    "                    response_clean = response['choices'][0]['text'].split(':\\n\\t\\t')[1:]\n",
    "\n",
    "                    for sentence_index,sub_response in enumerate(response_clean):\n",
    "                        sub_response = sub_response.split(\"\\n\\t\\t\")\n",
    "                        sub_label = []\n",
    "                        if story_index==2 and sentence_index <4:\n",
    "#                             print(\"SKIP at {}\".format(sentence_index))\n",
    "                            continue\n",
    "                        if 'pass\\n' in sub_response[0]:\n",
    "                            continue\n",
    "                        else:\n",
    "                            for entity_index,state_response in enumerate(sub_response):\n",
    "                                if entity_index == len(sub_response)-1:  ### the last item in slash n\n",
    "                                    state_response = state_response.split(\"\\n\")[0]\n",
    "                                entity_name = state_response.split('.')[0].strip()\n",
    "                                if human_flag:\n",
    "                                    if entity_name not in Name_list:\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        entity_attribute = state_response.split('.')[1]\n",
    "                                        if entity_attribute==attribute and entity_name in entity_list:\n",
    "                                            sentence = sentence_list[sentence_index]\n",
    "                                            attribute_label = singleAttributePrompting(sentence,entity_name,human_flag,output_log)\n",
    "                                            Pred_label[entity_list.index(entity_name)][sentence_index] = attribute_label\n",
    "                                else:\n",
    "                                    if entity_name in Name_list:\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        entity_attribute = state_response.split('.')[1]\n",
    "                                        if entity_attribute==attribute and entity_name in entity_list:\n",
    "                                            sentence = sentence_list[sentence_index]\n",
    "                                            attribute_label = singleAttributePrompting(sentence,entity_name,human_flag,output_log)\n",
    "                                            Pred_label[entity_list.index(entity_name)][sentence_index] = attribute_label\n",
    "                                        \n",
    "\n",
    "                    ###\n",
    "                    temp_pred_label[story_index] = Pred_label\n",
    "                count +=1\n",
    "                pbar.update(1)\n",
    "                for key,value in temp_pred_label.items():\n",
    "                    Pred_summary.append(value)\n",
    "                if count == len(activate_list):\n",
    "                    break\n",
    "\n",
    "            except Exception as e:\n",
    "                with open(output_log,\"a\") as file:\n",
    "                    file.write(\"reach limitation at {count}\\n\".format(count=count))\n",
    "                    file.write(\"{e}\\n\".format(e=e))\n",
    "                print(\"reach limitation at {count}\".format(count=count))\n",
    "                time.sleep(30)\n",
    "                continue\n",
    "                \n",
    "    return Pred_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe662a31",
   "metadata": {},
   "source": [
    "We use human attribute **conscious** as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "54835715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder exist\n",
      "Folder exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error! wait 30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:09<00:00, 64.85s/it]\n"
     ]
    }
   ],
   "source": [
    "summary_dict={}\n",
    "folder_path = 'ROC'\n",
    "state='effects'\n",
    "file_name= \"{}_{}\".format(folder_path,state)\n",
    "log_name=\"{}_{}_log\".format(folder_path,state)\n",
    "mkdir(file_name)\n",
    "mkdir(log_name)\n",
    "human_flag = False\n",
    "for attribute in ['conscious']:\n",
    "    \n",
    "# for attribute in ['clean']:\n",
    "    attribute_label_list = list(att_adj[attribute])\n",
    "    attribute_id = att_to_idx[attribute]\n",
    "    if attribute_id < 5:\n",
    "        human_flag = True\n",
    "    activate_list = np.arange(len(cskg_data))[:2]\n",
    "#     attribute_sen = get_attribute_dataset(tiered_dataset,Name_list,attribute_id,state)\n",
    "\n",
    "\n",
    "\n",
    "    if attribute == 'location':\n",
    "        Prompting = getProProming(attribute,'_',2,tiered_dataset,state)\n",
    "    else:\n",
    "        Prompting = getProProming(attribute,'_',6,tiered_dataset,state)\n",
    "    \n",
    "    \n",
    "    Pred_summary=PromptingPipe_ROC(roc_data,attribute,activate_list,attribute_label_list,log_name,human_flag)\n",
    "\n",
    "    output_file = os.path.join(file_name,attribute+\".npy\")\n",
    "    np.save(output_file,Pred_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "399782e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]]),\n",
       " array([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]]),\n",
       " array([[0., 0., 0., 0., 0.],\n",
       "        [0., 2., 0., 0., 0.]]),\n",
       " array([[0., 0., 0., 0., 0.],\n",
       "        [0., 2., 0., 0., 0.]])]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pred_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a4d28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
